{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heatmap/anaconda3/envs/fastai-cpu/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/home/heatmap/anaconda3/envs/fastai-cpu/lib/python3.6/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.structured import *\n",
    "\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "from pandas.api.types import is_string_dtype, is_object_dtype\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from IPython.display import display\n",
    "import feather\n",
    "from rfpimp import *\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pdpbox import pdp\n",
    "from plotnine import *\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.cluster import hierarchy as hc\n",
    "from sklearn import metrics\n",
    "from treeinterpreter import treeinterpreter as ti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/bulldozer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(f'{PATH}Train.csv', low_memory=False, parse_dates=['saledate'])\n",
    "df_test_raw= pd.read_csv(f'{PATH}Valid.csv', low_memory=False, parse_dates=['saledate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/bulldozer/feather', exist_ok=True)\n",
    "df = df.sort_values('saledate')\n",
    "n_valid = 12000  # same as Kaggle's test set size\n",
    "n_train = len(df)-n_valid\n",
    "df_train_raw = df[:n_train].reset_index(drop=True)\n",
    "df_valid_raw = df[n_train:].reset_index(drop=True)\n",
    "\n",
    "df_train_raw.to_feather('data/bulldozer/feather/train')\n",
    "df_valid_raw.to_feather('data/bulldozer/feather/valid')\n",
    "df_test_raw.to_feather('data/bulldozer/feather/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_norm(df):\n",
    "    for col in df.columns:\n",
    "        if is_string_dtype(df[col]) or is_object_dtype(df[col]):\n",
    "            df[col] = df[col].str.lower()\n",
    "            df[col] = df[col].fillna(np.nan) \n",
    "            df[col] = df[col].replace('none or unspecified', np.nan)\n",
    "            df[col] = df[col].replace('none', np.nan)\n",
    "            df[col] = df[col].replace('', np.nan)\n",
    "            \n",
    "def remove_inch(df, colname):\n",
    "    df[colname] = df[colname].str.extract(r'([0-9.]*)', expand=True)\n",
    "    df[colname] = pd.to_numeric(df[colname])            \n",
    "\n",
    "def clean(df):\n",
    "    del df['MachineID'] # dataset has inconsistencies\n",
    "    del df['SalesID']   # unique sales ID so not generalizer\n",
    "\n",
    "    df['auctioneerID'] = df['auctioneerID'].astype(str)\n",
    "\n",
    "    string_norm(df)\n",
    "\n",
    "    remove_inch(df, 'Tire_Size')\n",
    "    remove_inch(df, 'Undercarriage_Pad_Width')\n",
    "\n",
    "    df.loc[df['YearMade']<1950, 'YearMade'] = np.nan\n",
    "    df.loc[df.eval(\"saledate.dt.year < YearMade\"), 'YearMade'] = \\\n",
    "        df['saledate'].dt.year\n",
    "\n",
    "    df.loc[df.eval(\"MachineHoursCurrentMeter==0\"),\n",
    "           'MachineHoursCurrentMeter'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_order_product_size(df):\n",
    "    sizes = {np.nan:0, 'mini':1, 'compact':1, 'small':2, 'medium':3,\n",
    "             'large / medium':4, 'large':5}\n",
    "    df['ProductSize'] = df['ProductSize'].map(sizes).values\n",
    "    \n",
    "def onehot(df, colname):\n",
    "    ascat = df[colname].astype('category').cat.as_ordered()\n",
    "    onehot = pd.get_dummies(df[colname], prefix=colname, dtype=bool)\n",
    "    del df[colname]\n",
    "    df = pd.concat([df, onehot], axis=1)\n",
    "    return df, ascat.cat.categories\n",
    "\n",
    "def split_fiProductClassDesc(df):\n",
    "    df_split = df.fiProductClassDesc.str.split(' - ',expand=True).values\n",
    "    df['fiProductClassDesc'] = df_split[:,0] \n",
    "    df['fiProductClassSpec'] = df_split[:,1] \n",
    "    pattern = r'([0-9.\\+]*)(?: to ([0-9.\\+]*)|\\+) ([a-zA-Z ]*)'\n",
    "    spec = df['fiProductClassSpec']\n",
    "    df_split = spec.str.extract(pattern, expand=True).values\n",
    "    df['fiProductClassSpec_lower'] = pd.to_numeric(df_split[:,0])\n",
    "    df['fiProductClassSpec_upper'] = pd.to_numeric(df_split[:,1])\n",
    "    df['fiProductClassSpec_units'] = df_split[:,2]\n",
    "    del df['fiProductClassSpec'] \n",
    "    \n",
    "def prep_date(df, date_col):\n",
    "        df[\"saleYear\"] = df[date_col].dt.year\n",
    "        df[\"saleMonth\"] = df[date_col].dt.month\n",
    "        df[\"saleDay\"] = df[date_col].dt.day\n",
    "        df[\"saleDayofweek\"] = df[date_col].dt.dayofweek\n",
    "        df[\"saleWeekofyear\"] = df[date_col].dt.weekofyear\n",
    "        df[\"saleDayofyear\"] = df[date_col].dt.dayofyear\n",
    "        df[\"saleQuarter\"] = df[date_col].dt.quarter\n",
    "        df[\"saleIsWeekend\"] = np.where((df[date_col].dt.day_name == ['Sunday', 'Saturday']),1,0)\n",
    "        df[\"saleIsQuarterStart\"] = np.where(df[date_col].dt.month.isin([1, 4, 7, 10]),1 ,0)\n",
    "        df[\"saleIsQuarterEnd\"] = np.where(df[date_col].dt.month.isin([3, 6, 9, 12]),1 ,0)\n",
    "        df.drop(date_col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_eng(X): # for later use\n",
    "    prep_date(X, 'saledate')\n",
    "    df_order_product_size(X)\n",
    "    split_fiProductClassDesc(X)\n",
    "\n",
    "    X, hf_cats = onehot(X, 'Hydraulics_Flow')\n",
    "    # normalize categories first then one-hot encode\n",
    "    X['Enclosure'] = X['Enclosure'].replace('erops w ac', 'erops ac')\n",
    "    X['Enclosure'] = X['Enclosure'].replace('no rops', np.nan)\n",
    "    X, enc_cats = onehot(X, 'Enclosure')\n",
    "    catencoders = {'Hydraulics_Flow':hf_cats,\n",
    "                   'Enclosure':enc_cats}\n",
    "\n",
    "    return X, catencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_num (df, col):\n",
    "    df[col+'_na'] = pd.isnull(df[col]) \n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "def df_fix_missing_nums(df:pd.DataFrame) -> dict:\n",
    "    medians = {}  \n",
    "    for colname in df.columns:\n",
    "        if is_numeric_dtype(df[colname]):\n",
    "            medians[colname] = df[colname].median(skipna=True)\n",
    "            fix_missing_num(df, colname)\n",
    "    return medians\n",
    "\n",
    "def df_string_to_cat(df:pd.DataFrame) -> dict:\n",
    "    catencoders = {}\n",
    "    for colname in df.columns:\n",
    "        if is_string_dtype(df[colname]) or is_object_dtype(df[colname]):\n",
    "            df[colname] = df[colname].astype('category').cat.as_ordered()\n",
    "            catencoders[colname] = df[colname].cat.categories\n",
    "    return catencoders\n",
    "\n",
    "def df_cat_to_catcode(df):\n",
    "    for col in df.columns:\n",
    "        if is_categorical_dtype(df[col]):\n",
    "            df[col] = df[col].cat.codes + 1\n",
    "\n",
    "\n",
    "def numericalize(X, catencoders):\n",
    "    medians = df_fix_missing_nums(X)            \n",
    "    e = df_string_to_cat(X)\n",
    "    catencoders.update(e)\n",
    "    df_cat_to_catcode(X)    \n",
    "    return medians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Prepare training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"data/bulldozer/feather/train\")\n",
    "df = df.iloc[-100_000:] \n",
    "X_train, y_train = df.drop('SalePrice', axis=1), df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log(y_train)\n",
    "clean(X_train)\n",
    "X_train, catencoders = feature_eng(X_train)\n",
    "medians = numericalize(X_train, catencoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Prepare the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to problems like data leakage and to ensure consistency we will fill NaNs in the validation/test sets with the medians from our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_fix_missing_test_nums(df_test, medians):\n",
    "    for colname in medians:\n",
    "        df_test[colname+'_na'] = pd.isnull(df_test[colname])\n",
    "        df_test[colname].fillna(medians[colname], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we must ensure that our validation/test set consists of the exact same categories as our training set. Categories which are in the validation/test set but not in the training set will be encoded as NaNs and later as zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_apply_cats(df_test:pd.DataFrame, catencoders:dict):\n",
    "    for colname,encoder in catencoders.items():\n",
    "        # encode with categories from training set\n",
    "        df_test[colname] = \\\n",
    "            pd.Categorical(df_test[colname],\n",
    "                           categories=encoder, ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same goes for one-hot encoded columns. If a column in the validation data has more levels than the same column in the training data we end up with a validation set with more columns than the training set. To ensure this does not happen categories which are only in the validation/test set will be coded as NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_apply_cats(df_test, colname, catencoders_oh):\n",
    "    df_test[colname] = \\\n",
    "        pd.Categorical(df_test[colname],\n",
    "                       categories=catencoders[colname],\n",
    "                       ordered=True)\n",
    "    onehot = pd.get_dummies(df_test[colname], prefix=colname, dtype=bool)\n",
    "    del df_test[colname]\n",
    "    df_test = pd.concat([df_test, onehot], axis=1)\n",
    "    del catencoders[colname] # simplify df_apply_cats()\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted feature engeneering function for the validation/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng_test(df_test, catencoders):\n",
    "    prep_date(df_test, 'saledate')\n",
    "    df_order_product_size(df_test)\n",
    "    split_fiProductClassDesc(df_test)\n",
    "\n",
    "    df_test = onehot_apply_cats(df_test, 'Hydraulics_Flow', catencoders)\n",
    "    df_test['Enclosure'] = df_test['Enclosure'].replace('erops w ac', 'erops ac')\n",
    "    df_test['Enclosure'] = df_test['Enclosure'].replace('no rops', np.nan)\n",
    "    df_test = onehot_apply_cats(df_test, 'Enclosure', catencoders)\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And numericalize everything as a last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_test(df_test:pd.DataFrame, medians:dict, catencoders:dict):\n",
    "    df_apply_cats(df_test, catencoders)\n",
    "    df_fix_missing_test_nums(df_test, medians)\n",
    "    df_cat_to_catcode(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just have to apply the functions above to our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_feather(\"data/bulldozer/feather/valid\")\n",
    "X_valid, y_valid = df_valid.drop('SalePrice', axis=1), df_valid['SalePrice']\n",
    "\n",
    "y_valid = np.log(y_valid)\n",
    "clean(X_valid)\n",
    "X_valid = feature_eng_test(X_valid, catencoders)\n",
    "numericalize_test(X_valid, medians, catencoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our train and validation sets line up cocorrectly we can run a quick sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(df):\n",
    "    for col in df.columns:\n",
    "        if is_string_dtype(df[col]) or is_object_dtype(df[col]):\n",
    "            print(f\"Col {col} is still a string\")\n",
    "        if df[col].isnull().any():\n",
    "            print(f\"Col {col} still has missing values\")\n",
    "\n",
    "def check_types(df1,df2):\n",
    "    if df1.shape[1] != df2.shape[1]:\n",
    "        print(f\"Num columns differs: {df1.shape[1]} != {df2.shape[1]}\")\n",
    "    cols1 = set(df1.columns)\n",
    "    cols2 = set(df2.columns)\n",
    "    if cols1 != cols2:\n",
    "        print(f\"Column names differ:\")\n",
    "        if len(cols1-cols2)>0:\n",
    "            print(f\"\\tIn df1 not df2: {cols1-cols2}\")\n",
    "        if len(cols2-cols1)>0:\n",
    "            print(f\"\\tIn df2 not df1: {cols2-cols1}\")\n",
    "    for col in cols1.intersection(cols2): # check those in common\n",
    "        if df1[col].dtype != df2[col].dtype:\n",
    "            print(f\"Col {col} dtypes differ {df1[col].dtype} != {df2[col].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(X_train)\n",
    "sanity_check(X_valid)\n",
    "check_types(X_train, X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we just have to make sure that the columns in training and validations sets have the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = X_valid.reindex(columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m):\n",
    "    res = [\"Train_RMSLE:\", rmse(m.predict(X_train), y_train), \"Test_RMSLE:\",rmse(m.predict(X_valid), y_valid),\n",
    "               \"R²_Train:\", m.score(X_train, y_train), \"R²_Test:\", m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(['OOB: ', m.oob_score_])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Train_RMSLE:', 0.12404969772314627, 'Test_RMSLE:', 0.23797961327470485, 'R²_Train:', 0.9691045334838347, 'R²_Test:', 0.8916507983514057, ['OOB: ', 0.9150222850602242]]\n"
     ]
    }
   ],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features=0.5,\n",
    "                          n_jobs=-1, oob_score=True)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Subsetting the data for faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tuning we will work with a smaller subset of our data to speed up training time. With new data we first have to prepare train and validation set in the same manner as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"data/bulldozer/feather/train\")\n",
    "df = df.query('saledate.dt.year>=2007').copy()\n",
    "X_train, y_train = df.drop('SalePrice', axis=1), df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172411, 52)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log(y_train)\n",
    "clean(X_train)\n",
    "X_train, catencoders = feature_eng(X_train)\n",
    "medians = numericalize(X_train, catencoders)\n",
    "\n",
    "df_valid = pd.read_feather(\"data/bulldozer/feather/valid\")\n",
    "X_valid, y_valid = df_valid.drop('SalePrice', axis=1), df_valid['SalePrice']\n",
    "\n",
    "y_valid = np.log(y_valid)\n",
    "clean(X_valid)\n",
    "X_valid = feature_eng_test(X_valid, catencoders)\n",
    "df_apply_cats(X_valid, catencoders)\n",
    "df_fix_missing_test_nums(X_valid, medians)\n",
    "df_cat_to_catcode(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(X_train)\n",
    "sanity_check(X_valid)\n",
    "check_types(X_train, X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Train, measure and report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_valid(X_train, y_train, X_valid, y_valid, n_estimators=200,\n",
    "               max_features='auto', min_samples_leaf=1):\n",
    "    X_valid = X_valid.reindex(columns=X_train.columns)\n",
    "    m = RandomForestRegressor(n_estimators=n_estimators,\n",
    "                               n_jobs=-1,\n",
    "                               oob_score=True,\n",
    "                               max_features=max_features, \n",
    "                               min_samples_leaf=min_samples_leaf)\n",
    "    m.fit(X_train, y_train)\n",
    "    y_pred = m.predict(X_valid)\n",
    "    mae_valid = mean_absolute_error(np.exp(y_valid), np.exp(y_pred))\n",
    "    rmsle_valid = np.sqrt( mean_squared_error(y_valid, y_pred))\n",
    "    r2_score_valid = m.score(X_valid, y_valid)\n",
    "    print(f\"OOB R^2 {m.oob_score_:.5f}\")\n",
    "    print(f\"Validation R^2 {r2_score_valid:.5f}, RMSLE {rmsle_valid:.5f}, MAE ${mae_valid:.0f}\")\n",
    "    return m, r2_score_valid, rmsle_valid, mae_valid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=200, max_features=0.1, min_samples_leaf=1\n",
      "OOB R^2 0.90850\n",
      "Validation R^2 0.87832, RMSLE 0.25220, MAE $6436\n",
      "n_estimators=200, max_features=0.2, min_samples_leaf=1\n",
      "OOB R^2 0.92004\n",
      "Validation R^2 0.89127, RMSLE 0.23840, MAE $5915\n",
      "n_estimators=200, max_features=0.3, min_samples_leaf=1\n",
      "OOB R^2 0.92247\n",
      "Validation R^2 0.89409, RMSLE 0.23529, MAE $5784\n",
      "n_estimators=200, max_features=0.4, min_samples_leaf=1\n",
      "OOB R^2 0.92304\n",
      "Validation R^2 0.89541, RMSLE 0.23382, MAE $5701\n",
      "n_estimators=200, max_features=0.5, min_samples_leaf=1\n",
      "OOB R^2 0.92283\n",
      "Validation R^2 0.89535, RMSLE 0.23389, MAE $5678\n",
      "n_estimators=200, max_features=0.6, min_samples_leaf=1\n",
      "OOB R^2 0.92228\n",
      "Validation R^2 0.89513, RMSLE 0.23412, MAE $5674\n"
     ]
    }
   ],
   "source": [
    "ntrees = 200\n",
    "minleaf = 1\n",
    "for maxf in np.arange(.1,.7,.1):\n",
    "    print(f\"n_estimators={ntrees}, max_features={maxf:.1f}, min_samples_leaf={minleaf}\")\n",
    "    test_valid(X_train, y_train, X_valid, y_valid,\n",
    "               max_features=maxf, min_samples_leaf=minleaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with max_features= 0.5 performs best. So we will keep it steady while optimizing min_samples_leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=200, max_features=0.5, min_samples_leaf=2\n",
      "OOB R^2 0.92128\n",
      "Validation R^2 0.89546, RMSLE 0.23376, MAE $5683\n",
      "n_estimators=200, max_features=0.5, min_samples_leaf=3\n",
      "OOB R^2 0.91891\n",
      "Validation R^2 0.89517, RMSLE 0.23409, MAE $5708\n",
      "n_estimators=200, max_features=0.5, min_samples_leaf=4\n",
      "OOB R^2 0.91675\n",
      "Validation R^2 0.89483, RMSLE 0.23447, MAE $5729\n",
      "n_estimators=200, max_features=0.5, min_samples_leaf=5\n",
      "OOB R^2 0.91512\n",
      "Validation R^2 0.89453, RMSLE 0.23480, MAE $5741\n",
      "n_estimators=200, max_features=0.5, min_samples_leaf=6\n",
      "OOB R^2 0.91325\n",
      "Validation R^2 0.89360, RMSLE 0.23583, MAE $5774\n"
     ]
    }
   ],
   "source": [
    "maxf = .5\n",
    "for minleaf in range(2,7):\n",
    "   print(f\"n_estimators={ntrees}, max_features={maxf}, min_samples_leaf={minleaf}\")\n",
    "   test_valid(X_train, y_train, X_valid, y_valid,\n",
    "              max_features=maxf, min_samples_leaf=minleaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the best performance is achieved with min_samples_leaf = 2 or 3 depending on the run(According to the law of large numbers the fluctuation could be reduced by increasing the number of trees, the more trees the more stable are the results.). We can now train the model with the full data set again and see how we perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"data/bulldozer/feather/train\")\n",
    "X_train, y_train = df.drop('SalePrice', axis=1), df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log(y_train)\n",
    "clean(X_train)\n",
    "X_train, catencoders = feature_eng(X_train)\n",
    "medians = numericalize(X_train, catencoders)\n",
    "\n",
    "df_valid = pd.read_feather(\"data/bulldozer/feather/valid\")\n",
    "X_valid, y_valid = df_valid.drop('SalePrice', axis=1), df_valid['SalePrice']\n",
    "\n",
    "y_valid = np.log(y_valid)\n",
    "clean(X_valid)\n",
    "X_valid = feature_eng_test(X_valid, catencoders)\n",
    "df_apply_cats(X_valid, catencoders)\n",
    "df_fix_missing_test_nums(X_valid, medians)\n",
    "df_cat_to_catcode(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB R^2 0.91714\n",
      "Validation R^2 0.89722, RMSLE 0.23178, MAE $5663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RandomForestRegressor(max_features=0.5, min_samples_leaf=2, n_estimators=200,\n",
       "                       n_jobs=-1, oob_score=True),\n",
       " 0.8972222387265705,\n",
       " 0.23178027478410357,\n",
       " 5662.904534915931)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_valid(X_train, y_train, X_valid, y_valid,\n",
    "              max_features=0.5, min_samples_leaf=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While random forests are fairly robust and don't care that much on how many features they are trained we might improve performance by removing unimportant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X_train, y_train, X_valid, y_valid, drop=0.10):\n",
    "   min_rmsle = 99999\n",
    "   X_valid = X_valid.reindex(columns=X_train.columns)\n",
    "   m, _, rmsle, _ = test_valid(X_train, y_train, X_valid, y_valid,\n",
    "                                max_features=.4, min_samples_leaf=2)\n",
    "   I = importances(m, X_valid, y_valid)\n",
    "   features = list(I.index)\n",
    "   keep = best_features = features\n",
    "   n = int(.9/drop) # how many iterations? get to 90%\n",
    "   for i in range(1,n+1):\n",
    "       X2_train = X_train[keep]\n",
    "       X_valid2 = X_valid[keep]\n",
    "       print(f\"\\nNum features = {len(keep)}\")\n",
    "       m2, _, rmsle, _ = test_valid(X2_train, y_train, X_valid2, y_valid,\n",
    "                                     max_features=.4, min_samples_leaf=2)\n",
    "       if rmsle < min_rmsle:\n",
    "           min_rmsle = rmsle\n",
    "           best_features = keep\n",
    "       I2 = importances(m2, X_valid2, y_valid) # recompute since collinear\n",
    "       features = list(I2.index)\n",
    "       keep = features[0:int(len(features)*(1-drop))]\n",
    "\n",
    "   return min_rmsle, best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB R^2 0.91665\n",
      "Validation R^2 0.89674, RMSLE 0.23233, MAE $5701\n",
      "\n",
      "Num features = 89\n",
      "OOB R^2 0.91648\n",
      "Validation R^2 0.89733, RMSLE 0.23166, MAE $5685\n",
      "\n",
      "Num features = 84\n",
      "OOB R^2 0.91665\n",
      "Validation R^2 0.89999, RMSLE 0.22864, MAE $5587\n",
      "\n",
      "Num features = 79\n",
      "OOB R^2 0.91654\n",
      "Validation R^2 0.89960, RMSLE 0.22908, MAE $5591\n",
      "\n",
      "Num features = 75\n",
      "OOB R^2 0.91650\n",
      "Validation R^2 0.89984, RMSLE 0.22881, MAE $5583\n",
      "\n",
      "Num features = 71\n",
      "OOB R^2 0.91645\n",
      "Validation R^2 0.89989, RMSLE 0.22875, MAE $5572\n",
      "\n",
      "Num features = 67\n",
      "OOB R^2 0.91241\n",
      "Validation R^2 0.90194, RMSLE 0.22639, MAE $5457\n",
      "\n",
      "Num features = 63\n",
      "OOB R^2 0.91147\n",
      "Validation R^2 0.90255, RMSLE 0.22570, MAE $5443\n",
      "\n",
      "Num features = 59\n",
      "OOB R^2 0.91134\n",
      "Validation R^2 0.90247, RMSLE 0.22578, MAE $5444\n",
      "\n",
      "Num features = 56\n",
      "OOB R^2 0.84960\n",
      "Validation R^2 0.89540, RMSLE 0.23383, MAE $5705\n",
      "\n",
      "Num features = 53\n",
      "OOB R^2 0.84983\n",
      "Validation R^2 0.89649, RMSLE 0.23260, MAE $5688\n",
      "\n",
      "Num features = 50\n",
      "OOB R^2 0.84988\n",
      "Validation R^2 0.89565, RMSLE 0.23354, MAE $5692\n",
      "\n",
      "Num features = 47\n",
      "OOB R^2 0.84960\n",
      "Validation R^2 0.89585, RMSLE 0.23332, MAE $5701\n",
      "\n",
      "Num features = 44\n",
      "OOB R^2 0.84952\n",
      "Validation R^2 0.89600, RMSLE 0.23315, MAE $5697\n",
      "\n",
      "Num features = 41\n",
      "OOB R^2 0.84965\n",
      "Validation R^2 0.89652, RMSLE 0.23257, MAE $5683\n",
      "\n",
      "Num features = 38\n",
      "OOB R^2 0.84966\n",
      "Validation R^2 0.89582, RMSLE 0.23336, MAE $5704\n",
      "\n",
      "Num features = 36\n",
      "OOB R^2 0.84946\n",
      "Validation R^2 0.89594, RMSLE 0.23322, MAE $5695\n",
      "\n",
      "Num features = 34\n",
      "OOB R^2 0.84869\n",
      "Validation R^2 0.89641, RMSLE 0.23269, MAE $5702\n",
      "\n",
      "Num features = 32\n",
      "OOB R^2 0.84810\n",
      "Validation R^2 0.89585, RMSLE 0.23333, MAE $5715\n",
      "63 features is best:\n",
      "['YearMade', 'ProductSize', 'fiSecondaryDesc', 'fiProductClassSpec_lower', 'fiProductClassSpec_upper', 'Hydraulics_Flow_standard', 'fiModelDesc', 'ModelID', 'fiBaseModel', 'fiProductClassSpec_units', 'Enclosure_erops ac', 'fiModelDescriptor', 'YearMade_na', 'fiModelSeries', 'Engine_Horsepower', 'ProductGroupDesc', 'ProductGroup', 'Hydraulics', 'MachineHoursCurrentMeter', 'fiProductClassSpec_lower_na', 'fiProductClassDesc', 'Drive_System', 'Enclosure_orops', 'Ripper', 'fiProductClassSpec_upper_na', 'state', 'datasource', 'Transmission', 'Stick', 'Track_Type', 'Grouser_Type', 'Enclosure_erops', 'Tire_Size_na', 'Coupler', 'Ride_Control', 'Tire_Size', 'Travel_Controls', 'Pattern_Changer', 'Forks', 'Stick_Length', 'Steering_Controls', 'Thumb', 'Differential_Type', 'Blade_Width', 'saleMonth_na', 'ModelID_na', 'saleIsQuarterStart_na', 'Enclosure_erops ac_na', 'Enclosure_erops_na', 'Enclosure_orops_na', 'saleWeekofyear_na', 'saleYear', 'saleQuarter_na', 'saleDay_na', 'datasource_na', 'saleYear_na', 'Hydraulics_Flow_high flow_na', 'saleDayofyear_na', 'Backhoe_Mounting', 'ProductSize_na', 'saleIsWeekend', 'saleIsQuarterEnd_na', 'Hydraulics_Flow_standard_na']\n"
     ]
    }
   ],
   "source": [
    "min_rmsle, best_features = \\\n",
    "    select_features(X_train, y_train, X_valid, y_valid, drop=0.05)\n",
    "print(f\"{len(best_features)} features is best:\")\n",
    "print(best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB R^2 0.91161\n",
      "Validation R^2 0.90185, RMSLE 0.22651, MAE $5446\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train[best_features]\n",
    "X_valid = X_valid[best_features]\n",
    "rf, r2_score_bestf, rmsle_bestf, mae_bestf = \\\n",
    "    test_valid(X_train, y_train, X_valid, y_valid,\n",
    "               max_features=.5, min_samples_leaf=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With hyperparameter tuning and feature selection we went from a RMSLE of .240 down to .225."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Adjust for inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model underpredicts by $1657, 0.01699 log(dollars)\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred = rf.predict(X_valid)\n",
    "underprediction = np.mean(y_valid-y_valid_pred)\n",
    "dollars = np.mean(np.exp(y_valid)-np.exp(y_valid_pred))\n",
    "print(f\"Model underpredicts by ${dollars:.0f}, {underprediction:.5f} log(dollars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted-model validation R^2 0.90240, RMSLE 0.22587, MAE 5391\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred = rf.predict(X_valid) + underprediction\n",
    "mae_best = mean_absolute_error(np.exp(y_valid), np.exp(y_valid_pred))\n",
    "rmsle_best = np.sqrt( mean_squared_error(y_valid, y_valid_pred) )\n",
    "r2_score_best = r2_score(y_valid, y_valid_pred)\n",
    "print(f\"Adjusted-model validation R^2 {r2_score_best:.5f}, RMSLE {rmsle_best:.5f}, MAE {mae_best:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Final Model + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(f'{PATH}Train.csv', low_memory=False, parse_dates=['saledate'])\n",
    "df = df.query('saledate.dt.year>=2007').copy()\n",
    "X_train, y_train = df.drop('SalePrice', axis=1), df['SalePrice']\n",
    "y_train = np.log(y_train)\n",
    "clean(X_train)\n",
    "X_train, catencoders = feature_eng(X_train)\n",
    "medians = numericalize(X_train, catencoders)\n",
    "X_train = X_train[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_feather(\"data/bulldozer/feather/test\")\n",
    "X_test = df_test\n",
    "tmp = pd.read_csv(f'{PATH}ValidSolution.csv')\n",
    "y_test = np.log(tmp['SalePrice'])\n",
    "clean(X_test)\n",
    "X_test = feature_eng_test(X_test, catencoders)\n",
    "df_apply_cats(X_test, catencoders)\n",
    "df_fix_missing_test_nums(X_test, medians)\n",
    "df_cat_to_catcode(X_test)\n",
    "X_test = X_test[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(X_train)\n",
    "sanity_check(X_valid)\n",
    "check_types(X_train, X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11573, 63), (11573,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB R^2 0.91478\n",
      "Validation R^2 0.89657, RMSLE 0.23620, MAE $5831\n"
     ]
    }
   ],
   "source": [
    "m, r2_score_test, rmsle_test, mae_test = \\\n",
    "    test_valid(X_train, y_train + underprediction,\n",
    "               X_test, y_test,\n",
    "               max_features=.5, min_samples_leaf=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Tree interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Bulldozer Final.ipynb",
    "public": false
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
